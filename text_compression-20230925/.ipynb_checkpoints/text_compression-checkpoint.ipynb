{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested imports. You may add your own here.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Other Imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text compression assignment\n",
    "\n",
    "It is said that you can measure the intelligence of an AI from the amount it can compress a text without information loss.\n",
    "One way to think about this is that, the more a text is predictable, the more words we can leave out of it as we can guess the missing words.\n",
    "On the other hand, the more intelligent an AI is, the more it will find texts to be predictable and so the more words it can leave out and guess.\n",
    "This has led to a competition called the [Hutter Prize](http://prize.hutter1.net/) where the objective is to compress a given text as much as possible.\n",
    "The record for compressing a 1GB text file extracted from a Wikipedia snapshot is about 115MB.\n",
    "The main hurdle here is that the program used to decompress the file must be treated as part of the compressed file, meaning that the program itself must also be small.\n",
    "\n",
    "In this assignment, you're going to be doing something similar using a smaller text file and using neural language models to guess missing words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data processing (10%)\n",
    "\n",
    "You have a train/dev/test split corpus of text from Wikipedia consisting of single sentences.\n",
    "Each sentence is on a separate line and each sentence has been tokenised for you such that tokens are space separated.\n",
    "This means that you only need to split by space to get the tokens.\n",
    "The text has all been lowercased as well.\n",
    "The objective here is to be able to compress the text losslessly, meaning that it can be decompressed back to the original string:\n",
    "\n",
    "$$\\text{decompress}(\\text{compress}(t)) = t$$\n",
    "\n",
    "Do not do any further pre-processing on the text (such as stemming) as it may result in unrecoverable information loss.\n",
    "The test set is what we will be compressing and will not be processed at all as it will be treated as a single big string by the compression/decompression algorithms.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Load the train set and dev set text files into a list of sentences where each sentence is tokenised (by splitting by space).\n",
    "Do not load the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Train Sentences:\n",
      "\n",
      " [['dr.', 'tonie', 'mcdonald', 'is', 'a', 'life', 'long', 'levittown', 'resident', 'who', 'taught', 'and', 'rose', 'through', 'the', 'ranks', 'of', 'the', 'district', 'she', 'now', 'leads', '.'], ['he', 'received', 'his', 'ba', 'in', 'chemistry', ',', 'magna', 'cum', 'laude', ',', 'from', 'amherst', 'college', 'in', '1', '9', '8', '1', '.'], ['the', 'growth', 'of', 'twin', 'cities', 'international', 'airport', 'during', 'the', 'last', 'half', 'of', 'the', 'twentieth', 'century', 'along', 'with', 'additions', 'of', 'land', 'to', 'the', 'fort', 'snelling', 'compound', 'meant', 'further', 'reductions', '.']]\n",
      "\n",
      "Sample Dev Sentences:\n",
      "\n",
      " [['jones', 'viewed', 'the', 'resolution', 'as', 'the', 'framework', ',', 'and', 'not', 'the', 'final', 'solution', ',', 'for', 'enabling', 'librarians', 'to', 'confront', 'issues', 'that', 'hampered', '``', 'human', 'freedom', \"''\", '.'], ['he', 'lives', 'in', 'ottawa', ',', 'where', 'he', 'composes', 'full-time', '.'], ['many', 'natural', 'gas', 'pipelines', 'span', 'the', 'country', \"'s\", 'territory', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Function to load and tokenize sentences from a text file\n",
    "def load_and_tokenize(file_path):\n",
    "    # Open the file with read permissions and UTF-8 encoding\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Read all lines from the file into a list\n",
    "        sentences = f.readlines()\n",
    "    # Return a list of tokenized sentences\n",
    "    # Each sentence is stripped of leading/trailing whitespaces and then split by spaces\n",
    "    return [sentence.strip().split(' ') for sentence in sentences]\n",
    "\n",
    "# Define the paths to the train and dev text files\n",
    "train_file_path = 'train.txt'\n",
    "dev_file_path = 'dev.txt'\n",
    "\n",
    "# Load and tokenize the sentences from the train set\n",
    "train_sentences = load_and_tokenize(train_file_path)\n",
    "# Load and tokenize the sentences from the dev set\n",
    "dev_sentences = load_and_tokenize(dev_file_path)\n",
    "\n",
    "# Take the first 3 tokenized sentences from the train set as sample data\n",
    "sample_train = train_sentences[:3]\n",
    "# Take the first 3 tokenized sentences from the dev set as sample data\n",
    "sample_dev = dev_sentences[:3]\n",
    "\n",
    "# Print the sample data from the train set to verify the output\n",
    "print(\"Sample Train Sentences:\\n\\n\", sample_train)\n",
    "# Print the sample data from the dev set to verify the output\n",
    "print(\"\\nSample Dev Sentences:\\n\\n\", sample_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Extract a vocabulary consisting of the tokens that occur at least 3 times in the train set and output the size of your vocabulary.\n",
    "Also output the most frequent vocabulary token in the train set, which should be 'the'.\n",
    "Include the edge token, unknown token, and pad token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7874\n",
      "\n",
      "Most Frequent Token: the\n"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each token in the train set\n",
    "# This uses a nested list comprehension to flatten the list of sentences into a list of tokens\n",
    "# The Counter class then counts the occurrences of each token\n",
    "token_counter = Counter(token for sentence in train_sentences for token in sentence)\n",
    "\n",
    "# Create a vocabulary by filtering out tokens that appear less than 3 times in the train set\n",
    "# This is done using a set comprehension\n",
    "vocab = {token for token, count in token_counter.items() if count >= 3}\n",
    "\n",
    "# Define a set of special tokens: Padding, Unknown, and Edge tokens\n",
    "special_tokens = {'<PAD>', '<UNK>', '<EDGE>'}\n",
    "\n",
    "# Add the special tokens to the vocabulary\n",
    "vocab.update(special_tokens)\n",
    "\n",
    "# Testing: Check if special tokens are in the vocabulary\n",
    "assert '<PAD>' in vocab, \"'<PAD>' not found in vocab\"\n",
    "assert '<UNK>' in vocab, \"'<UNK>' not found in vocab\"\n",
    "assert '<EDGE>' in vocab, \"'<EDGE>' not found in vocab\"\n",
    "\n",
    "# Calculate the size of the vocabulary by taking the length of the vocab set\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Find the most frequent token in the train set\n",
    "# The most_common(1) method returns a list with a single tuple, hence [0][0] to get the token\n",
    "most_frequent_token = token_counter.most_common(1)[0][0]\n",
    "\n",
    "# Output the vocabulary size and the most frequent token for verification\n",
    "print(f\"Vocabulary Size: {vocab_size}\\n\")\n",
    "print(f\"Most Frequent Token: {most_frequent_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) Process the loaded token sequences for the train set and dev set using the vocabulary created above in a way that is suitable for a language model, making use of edge tokens, unknown tokens, and pad tokens.\n",
    "Do not do this for the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Processed Train Sentences:\n",
      "\n",
      " [['<EDGE>', 'dr.', '<UNK>', 'mcdonald', 'is', 'a', 'life', 'long', '<UNK>', 'resident', 'who', 'taught', 'and', 'rose', 'through', 'the', 'ranks', 'of', 'the', 'district', 'she', 'now', 'leads', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>'], ['<EDGE>', 'he', 'received', 'his', 'ba', 'in', 'chemistry', ',', '<UNK>', '<UNK>', '<UNK>', ',', 'from', '<UNK>', 'college', 'in', '1', '9', '8', '1', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>'], ['<EDGE>', 'the', 'growth', 'of', 'twin', 'cities', 'international', 'airport', 'during', 'the', 'last', 'half', 'of', 'the', 'twentieth', 'century', 'along', 'with', '<UNK>', 'of', 'land', 'to', 'the', 'fort', '<UNK>', 'compound', 'meant', 'further', 'reductions', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>']]\n",
      "\n",
      "Sample Processed Dev Sentences:\n",
      "\n",
      " [['<EDGE>', 'jones', 'viewed', 'the', 'resolution', 'as', 'the', 'framework', ',', 'and', 'not', 'the', 'final', 'solution', ',', 'for', '<UNK>', '<UNK>', 'to', '<UNK>', 'issues', 'that', '<UNK>', '``', 'human', 'freedom', \"''\", '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>'], ['<EDGE>', 'he', 'lives', 'in', '<UNK>', ',', 'where', 'he', '<UNK>', 'full-time', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>'], ['<EDGE>', 'many', 'natural', 'gas', '<UNK>', '<UNK>', 'the', 'country', \"'s\", 'territory', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>']]\n"
     ]
    }
   ],
   "source": [
    "# Function to process and pad sentences for training a language model\n",
    "def process_sentences_dynamic(sentences, vocab):\n",
    "    # Find the length of the longest sentence\n",
    "    max_length = max(len(sentence) for sentence in sentences) + 2  # +2 for the two EDGE tokens\n",
    "    \n",
    "    # Initialize an empty list to hold the processed sentences\n",
    "    processed = []  \n",
    "    for sentence in sentences:\n",
    "        # Add an initial edge token and replace tokens not in vocab with '<UNK>'\n",
    "        new_sentence = ['<EDGE>'] + [token if token in vocab else '<UNK>' for token in sentence]\n",
    "        \n",
    "        # Pad the sentence and then add an edge token\n",
    "        while len(new_sentence) < max_length - 1:  # Leave space for one edge token\n",
    "            new_sentence.append('<PAD>')\n",
    "        new_sentence.append('<EDGE>')  # Add the final edge token\n",
    "\n",
    "        # Add the processed and padded sentence to the list\n",
    "        processed.append(new_sentence)\n",
    "            \n",
    "    return processed\n",
    "\n",
    "# Process the train and dev sentences\n",
    "processed_train_sentences = process_sentences_dynamic(train_sentences, vocab)\n",
    "processed_dev_sentences = process_sentences_dynamic(dev_sentences, vocab)\n",
    "\n",
    "# Show some sample processed data for verification\n",
    "sample_processed_train = processed_train_sentences[:3]\n",
    "sample_processed_dev = processed_dev_sentences[:3]\n",
    "\n",
    "# Print the first 3 processed and padded sentences from both the train and dev sets\n",
    "print(\"Sample Processed Train Sentences:\\n\\n\", sample_processed_train)\n",
    "print(\"\\nSample Processed Dev Sentences:\\n\\n\", sample_processed_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) Finally, load the test set text file as single string and keep it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of Test Set: 218470 characters\n",
      "\n",
      "First 100 characters of Test Set:\n",
      "\n",
      " this coincidence enabled freemasons to wear the forget-me-not badge as a secret sign of membership .\n"
     ]
    }
   ],
   "source": [
    "# Function to load the test set into a single string\n",
    "def load_test_set(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        # Remove trailing whitespace, including newlines\n",
    "        return content.rstrip()  \n",
    "\n",
    "# Specify the path to the test text file\n",
    "test_file_path = 'test.txt'\n",
    "\n",
    "# Load the test set using the function\n",
    "test_set_string = load_test_set(test_file_path)\n",
    "\n",
    "# Output the total length of the test set string\n",
    "print(f\"\\nTotal length of Test Set: {len(test_set_string)} characters\\n\")\n",
    "\n",
    "# Output the first 100 characters of the test set for verification\n",
    "print(\"First 100 characters of Test Set:\\n\\n\", test_set_string[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Evaluation tools (10%)\n",
    "\n",
    "We're going to need a function that evaluates our language models as well as a way to test this function before we make the language model.\n",
    "To test the evaluation function, you need to make a mock model which can be used exactly like a language model but that works with some simple rules.\n",
    "This mock model will then be used to check the evaluation, compression, and decompression functions before we've developed the language model.\n",
    "\n",
    "In this assignment, a language model function assumes the following signature:\n",
    "\n",
    "* A parameter `x_indexes` being a tensor that gives the model's input token indexes of a batch of sentences, starting with the edge token.\n",
    "    The tensor is of type `int64` with shape `(batch size, time steps)`.\n",
    "* Returns a tensor of logits predicting which vocabulary token can be the next token after each token in `x_indexes`.\n",
    "    The tensor is of type `float32` with shape `(batch size, time steps, vocab size)`.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) Develop a mock language model.\n",
    "This language model will be a module that predicts the next token after every token using these rules:\n",
    "\n",
    "* If the actual previous token (not the predicted one) was 'the' then predict that the current token is 'dog'.\n",
    "* Otherwise, predict that the current token is 'the'.\n",
    "\n",
    "Remember that it is logits that will be returned by the forward function, not probabilities.\n",
    "**Give the token being predicted a logit of 2 and all other tokens a logit of 0.**\n",
    "The name of this class should be `MockModel`.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Feel free to use `for` loops and `if` statements.\n",
    "* Remember that `x_indexes` is a tensor of previous tokens.\n",
    "    For example, if `x_indexes` is `[[1, 3]]`, this is saying that the first token to predict has a previous token being 1 and the second token to predict has a previous token being 3.\n",
    "\n",
    "Some test code has been provided to check that your mock model is correct.\n",
    "Fix the test code as instructed in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_indexes: tensor([[ 642, 3390]])\n",
      "Logits: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a list from the vocabulary set\n",
    "vocab_list = list(vocab)\n",
    "# Create a dictionary that maps each token to its index in vocab_list\n",
    "token2index = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "\n",
    "# Define specific token indexes for easy reference in MockModel\n",
    "index_of_edge = token2index['<EDGE>']\n",
    "index_of_the = token2index['the']\n",
    "index_of_dog = token2index['dog']\n",
    "\n",
    "# Define the MockModel class for testing the compression and decompression functions\n",
    "class MockModel:\n",
    "    # Initialize the MockModel with vocabulary size and specific token indexes\n",
    "    def __init__(self, vocab_size, index_of_edge, index_of_the, index_of_dog):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.index_of_edge = index_of_edge\n",
    "        self.index_of_the = index_of_the\n",
    "        self.index_of_dog = index_of_dog\n",
    "\n",
    "    # Define the forward function to predict next tokens based on input indexes\n",
    "    def forward(self, x_indexes):\n",
    "        # Initialize logits tensor with zeros\n",
    "        logits = torch.zeros(*x_indexes.shape, self.vocab_size, dtype=torch.float32)\n",
    "        # Loop through each token in each sentence\n",
    "        for i, batch in enumerate(x_indexes):\n",
    "            for j, prev_token in enumerate(batch):\n",
    "                # If the previous token is 'the', predict 'dog'\n",
    "                if prev_token == self.index_of_the:\n",
    "                    logits[i, j, self.index_of_dog] = 2\n",
    "                # Otherwise, predict 'the'\n",
    "                else:\n",
    "                    logits[i, j, self.index_of_the] = 2\n",
    "        return logits\n",
    "\n",
    "    # Define the __call__ method to make MockModel callable\n",
    "    def __call__(self, x_indexes):\n",
    "        return self.forward(x_indexes)\n",
    "\n",
    "# Initialize the MockModel for testing\n",
    "model = MockModel(vocab_size, index_of_edge, index_of_the, index_of_dog)\n",
    "\n",
    "# Example input tensor for testing MockModel\n",
    "x_indexes = torch.tensor([[token2index['the'], token2index['dog']]])\n",
    "\n",
    "# Print out the input tensor for debugging\n",
    "print(f\"x_indexes: {x_indexes}\")\n",
    "\n",
    "# Run the MockModel to get the logits\n",
    "logits = model.forward(x_indexes)\n",
    "\n",
    "# Print out the logits for debugging\n",
    "print(f\"Logits: {logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Setup for the mock test\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Get specific token indexes for easy reference\n",
    "index_of_edge = token2index['<EDGE>']\n",
    "index_of_the = token2index['the']\n",
    "index_of_dog = token2index['dog']\n",
    "\n",
    "# Define the device for PyTorch (use 'cuda' for GPU)\n",
    "device = 'cpu'\n",
    "\n",
    "# Initialize the MockModel with the vocab_size and specific token indexes\n",
    "mock_model = MockModel(vocab_size, index_of_edge, index_of_the, index_of_dog)\n",
    "\n",
    "# Create mock test data as PyTorch tensor\n",
    "# Each row is a sequence of token indexes\n",
    "mock_x_indexes = torch.tensor([\n",
    "    [index_of_edge, index_of_dog, index_of_the, index_of_the],\n",
    "    [index_of_edge, index_of_the, index_of_dog, index_of_dog],\n",
    "], dtype=torch.int64, device=device)\n",
    "\n",
    "# Define expected logits based on the mock_model's logic\n",
    "# Initialize with zeros and set specific positions to 2\n",
    "mock_expected_logits = torch.zeros((2, 4, vocab_size), dtype=torch.float32, device=device)\n",
    "mock_expected_logits[0, 0, index_of_the] = 2\n",
    "mock_expected_logits[0, 1, index_of_the] = 2\n",
    "mock_expected_logits[0, 2, index_of_dog] = 2\n",
    "mock_expected_logits[0, 3, index_of_dog] = 2\n",
    "mock_expected_logits[1, 0, index_of_the] = 2\n",
    "mock_expected_logits[1, 1, index_of_dog] = 2\n",
    "mock_expected_logits[1, 2, index_of_the] = 2\n",
    "mock_expected_logits[1, 3, index_of_the] = 2\n",
    "\n",
    "# Run the mock_model on the mock test data to get logits\n",
    "mock_logits = mock_model(mock_x_indexes)\n",
    "\n",
    "# Assertions to check if the model output is as expected\n",
    "assert mock_logits.shape == mock_expected_logits.shape, 'Output shape is invalid.'\n",
    "assert mock_logits.dtype == mock_expected_logits.dtype, 'Output data type is invalid.'\n",
    "assert np.unique(mock_logits.detach().cpu().numpy()).tolist() == [0.0, 2.0], 'Output has values other than 0 and 2'\n",
    "assert (mock_logits == mock_expected_logits).all(), 'Output has the wrong logits.'\n",
    "\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) Next, we need a function that measures the perplexity of a language model on the dev set.\n",
    "Your function must take a model and a data set of token indexes and return the perplexity over the entire data set.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Don't forget that the perplexity includes the probability of the edge token at the end of the sentence.\n",
    "* Don't forget to ignore pad tokens.\n",
    "\n",
    "Use this function to find the mock model's perplexity on the dev set, which should be equal to `7062.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7062.261323736134\n"
     ]
    }
   ],
   "source": [
    "# Function to convert tokens in processed sentences to their corresponding indexes\n",
    "def sentences_to_index(sentences, token2index):\n",
    "    return [[token2index.get(token, token2index['<UNK>']) for token in sentence] for sentence in sentences]\n",
    "\n",
    "# Convert processed_dev_sentences to indexes using the function\n",
    "dev_data = sentences_to_index(processed_dev_sentences, token2index)\n",
    "\n",
    "# Convert the list of lists to a PyTorch tensor\n",
    "dev_data_tensors = [torch.tensor(sentence) for sentence in dev_data]\n",
    "\n",
    "# Pad sequences to the same length using PAD token\n",
    "dev_data_padded = pad_sequence(dev_data_tensors, batch_first=True, padding_value=token2index['<PAD>'])\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(model, dev_set, pad_token_id):\n",
    "    total_log_prob = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Loop through each sentence in the development set\n",
    "    for i in range(dev_set.size(0)):  # Iterate over each batch\n",
    "        sentence = dev_set[i].unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "        # Get the model's prediction as logits\n",
    "        logits = model(sentence)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Loop through each token in the sentence\n",
    "        for i in range(sentence.shape[1]):  # Include last token\n",
    "            target_token = sentence[0, i]\n",
    "\n",
    "            # Skip pad tokens and the first EDGE token\n",
    "            if i == 0 or target_token == pad_token_id:\n",
    "                continue\n",
    "\n",
    "            # Get the probability of the target token\n",
    "            target_prob = probabilities[0, i-1, target_token].item()\n",
    "\n",
    "            # Update total log probability\n",
    "            total_log_prob += math.log(target_prob)\n",
    "\n",
    "            # Update the total number of tokens\n",
    "            total_tokens += 1\n",
    "\n",
    "    # Calculate perplexity based on the total log probability and total tokens\n",
    "    perplexity = math.exp(-total_log_prob / total_tokens)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "# Define the ID for PAD token\n",
    "pad_token_id = token2index['<PAD>']\n",
    "\n",
    "# Calculate and print the perplexity\n",
    "perplexity = calculate_perplexity(model, dev_data_padded, pad_token_id)\n",
    "print(f'Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Compression and decompression (20%)\n",
    "\n",
    "We will now write the code that makes the actual compression and decompression of a text.\n",
    "\n",
    "The compression algorithm will work as follows:\n",
    "\n",
    "* You have a string of text to compress called `text` and a language model called `model`.\n",
    "* Extract a list of tokens from `text` called `tokens` and a list of corresponding token indexes called `indexes`.\n",
    "* Use `model` on `indexes` to produce `predicted`, a list of predicted next tokens for every index in `indexes`.\n",
    "    A predicted next token is just the most probable token according to `model`.\n",
    "* If a token in `predicted` corresponds to a token in `tokens`, then that token can be predicted by the model from its previous tokens.\n",
    "    In this case, we don't need to have the token written down as it can be predicted, so we replace it in `tokens` with the single letter 'X' to say that a token should be predicted here.\n",
    "    If 'X' is shorter than the replaced token, then the text will become shorter.\n",
    "    Since all the text in our data sets is in lowercase, there will never be an 'X' in a sentence, so we can safely use it as a flag.\n",
    "* If the token isn't correctly predicted then we leave the token in the text as-is.\n",
    "* After all predictable tokens in `tokens` have been replaced with an 'X', return `tokens` as a space separated string.\n",
    "\n",
    "The decompression algorithm will work as follows:\n",
    "\n",
    "* You have a string of compressed text called `text` and a language model called `model`.\n",
    "* Extract a list of tokens from `text` called `tokens`.\n",
    "* Go through the tokens in `tokens` from the front and stop at the first 'X'.\n",
    "* Convert all the tokens before the 'X' to token indexes called `indexes`.\n",
    "* Use `model` to predict what the most probable token at the end of `indexes` would be.\n",
    "* Replace the 'X' in `tokens` with this most probable token.\n",
    "* Repeat this for every 'X'.\n",
    "* After all 'X' are replaced in `tokens`, return `tokens` as a space separated string.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) Start with the compression function.\n",
    "The input text will consist of sentences separated by new lines and space separated tokens (just like the raw data sets).\n",
    "The function should return a single string with each line in the input text being compressed.\n",
    "Remember that we want a compressed text to be decompressed back into the exact original text, which means that all out-of-vocabulary tokens must be left as-is (**there must not be any unknown tokens in the output**).\n",
    "\n",
    "Print out the result of compressing this sentence using the mock model:\n",
    "\n",
    "`the dog bit the cat sensually .`\n",
    "\n",
    "which should be compressed into:\n",
    "\n",
    "`X X bit X cat sensually .`\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You don't need to follow the algorithm described above exactly (you can use different variable names and you can use new variables).\n",
    "* Don't forget that out of vocabulary tokens still need to be replaced with the unknown token when creating the token indexes.\n",
    "    What you can't do is return unknown tokens in the compressed output.\n",
    "* The most probable token index for all token positions at once can be found from the logits by using `.argmax(1)`.\n",
    "* Do not compare the token indexes of the uncompressed sentence to the predicted token indexes as otherwise the unknown token can be considered a correct prediction.\n",
    "    Instead, compare the predictions with the string tokens in the uncompressed sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed: X X bit X cat sensually.\n"
     ]
    }
   ],
   "source": [
    "# Compression Function\n",
    "def compress_text(text, model, token2index, index2token):\n",
    "    compressed_lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        # Add edge token at the beginning of each sentence\n",
    "        tokens = ['<EDGE>'] + line.split()  \n",
    "        \n",
    "        token_indexes = [token2index.get(token, token2index['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # MockModel expects a batch dimension, so we add one\n",
    "        input_indexes = torch.tensor([token_indexes[:-1]])  # Exclude the last token for prediction\n",
    "        \n",
    "        # Get predicted logits from the model\n",
    "        logits = model(input_indexes)\n",
    "        \n",
    "        # Get the most probable next tokens\n",
    "        predicted_indexes = logits.argmax(dim=-1).squeeze().tolist()\n",
    "        \n",
    "        compressed_tokens = []\n",
    "        for pred_idx, actual_token in zip(predicted_indexes, tokens[1:]):\n",
    "            pred_token = index2token[pred_idx]\n",
    "            if pred_token == actual_token:\n",
    "                compressed_tokens.append('X')\n",
    "            else:\n",
    "                compressed_tokens.append(actual_token)\n",
    "        \n",
    "        # Create the compressed line without the edge token\n",
    "        compressed_line = ' '.join(compressed_tokens)\n",
    "        compressed_lines.append(compressed_line)\n",
    "    \n",
    "    return '\\n'.join(compressed_lines)\n",
    "\n",
    "# Create an index to token mapping\n",
    "index2token = {idx: token for token, idx in token2index.items()}\n",
    "\n",
    "# Test the function\n",
    "test_text = \"the dog bit the cat sensually.\"\n",
    "compressed = compress_text(test_text, model, token2index, index2token)\n",
    "print(f\"Compressed: {compressed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) Now write the decompression function.\n",
    "Again, The input text will consist of sentences separated by new lines and space separated tokens, only this time, some of those tokens will be an 'X'.\n",
    "The function should return a single big string where each line in the compressed text is decompressed back into the original input line.\n",
    "\n",
    "Print out the result of decompressing the compressed text:\n",
    "\n",
    "`X X bit X cat sensually .`\n",
    "\n",
    "which should be decompressed into:\n",
    "\n",
    "`the dog bit the cat sensually .`\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You cannot use the language model once to predict all 'X's at once because the sentence prefix leading up to the 'X' must not have another 'X' in it.\n",
    "    So you have to make a separate language model prediction for every 'X' using only the tokens that come before 'X' as input to the language model (plus the edge token at the front).\n",
    "* Don't forget that the input to the language model cannot contain 'X's, so make sure that you're replacing those 'X's with their predicted token when constructing the language model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed: the dog bit the cat sensually.\n"
     ]
    }
   ],
   "source": [
    "# Decompression Function\n",
    "def decompress_text(compressed_text, model, token2index, index2token):\n",
    "    # Initialize an empty list to store decompressed lines\n",
    "    decompressed_lines = []\n",
    "    \n",
    "    # Loop through each line in the compressed text\n",
    "    for line in compressed_text.split(\"\\n\"):\n",
    "        # Tokenize the line into words\n",
    "        tokens = line.split()\n",
    "        \n",
    "        # Initialize the list of decompressed tokens with a starting token (e.g., '<s>')\n",
    "        decompressed_tokens = ['<s>']\n",
    "        \n",
    "        # Loop through tokens to decompress the line\n",
    "        for token in tokens:\n",
    "            if token == 'X':\n",
    "                # Convert tokens to indexes and add a batch dimension for the model\n",
    "                input_indexes = torch.tensor([token2index.get(t, token2index['<UNK>']) for t in decompressed_tokens])\n",
    "                input_indexes = input_indexes.unsqueeze(0)\n",
    "                \n",
    "                # Get logits for each token from the model\n",
    "                logits = model(input_indexes)\n",
    "                \n",
    "                # Get the index of the most probable next token\n",
    "                predicted_index = logits[0, -1, :].argmax().item()\n",
    "                \n",
    "                # Convert the index back to a token\n",
    "                predicted_token = index2token[predicted_index]\n",
    "                \n",
    "                # Add the predicted token to the list of decompressed tokens\n",
    "                decompressed_tokens.append(predicted_token)\n",
    "            else:\n",
    "                # If the token is not 'X', add it to the list of decompressed tokens as-is\n",
    "                decompressed_tokens.append(token)\n",
    "        \n",
    "        # Remove the initial edge token and convert the list back to a string\n",
    "        decompressed_line = ' '.join(decompressed_tokens[1:])\n",
    "        \n",
    "        # Add the decompressed line to the list of decompressed lines\n",
    "        decompressed_lines.append(decompressed_line)\n",
    "    \n",
    "    # Convert the list of decompressed lines back to a single string and return\n",
    "    return '\\n'.join(decompressed_lines)\n",
    "\n",
    "# Test the function with a sample compressed text\n",
    "compressed_text = \"X X bit X cat sensually.\"\n",
    "decompressed = decompress_text(compressed_text, model, token2index, index2token)\n",
    "print(f\"Decompressed: {decompressed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) Next, calculate and print the space saving amount of the mock model on the test set.\n",
    "The space saving amount is calculated as follows:\n",
    "\n",
    "$$\\text{space\\_saving}(t) = 1 - \\frac{|\\text{compress}(t)|}{|t|}$$\n",
    "\n",
    "where $|t|$ is the number of characters in text $t$.\n",
    "\n",
    "This measure tells you what fraction of the original size has been shaved off after compression (higher is better).\n",
    "The mock model should give 2.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space saving: 2.36%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the space saving percentage\n",
    "def calculate_space_saving(original_text, compressed_text):\n",
    "    # Get the length (in characters) of the original text\n",
    "    original_size = len(original_text)\n",
    "    # Get the length (in characters) of the compressed text\n",
    "    compressed_size = len(compressed_text)\n",
    "    # Calculate and return the space saving fraction\n",
    "    return 1 - (compressed_size / original_size)\n",
    "\n",
    "# Use the compress_text function to compress the test set text\n",
    "# This function was defined earlier and uses the model to predict and replace tokens\n",
    "compressed_test_set = compress_text(test_set_string, model, token2index, index2token)\n",
    "\n",
    "# Use the calculate_space_saving function to get the space saving percentage\n",
    "# It compares the size of the original and compressed texts\n",
    "space_saving = calculate_space_saving(test_set_string, compressed_test_set)\n",
    "\n",
    "# Print the space saving percentage, rounded to 2 decimal places\n",
    "print(f\"Space saving: {space_saving * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Making and using a language model (50%)\n",
    "\n",
    "Now we finally train a language model and use it to compress the test set.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1) Train a neural language model on the train set.\n",
    "After training, show a graph of how the *dev set perplexity* varies with each epoch (use the perplexity function you wrote above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2) Now measure the space saving amount of the trained model on the test text.\n",
    "Also check that when you decompress the compressed test text, you get exactly the same string as the test text.\n",
    "\n",
    "Note: You may need to strip off the new line character from the end of the test text when comparing it to the decompressed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3) Now you need to analyse the model's output.\n",
    "Split the test text into sentences and compress each individual sentence.\n",
    "Print out the top 5 most compressed sentences and the top 5 least compressed sentences according to the space saving metric together with the compressed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4) Is the reason for whether a sentence is compressible or not due to its similarity to the train set (a sentence that is similar to one in the train set would be easier to predict and thus more tokens will be compressed)?\n",
    "Find out the answer to this by doing the following:\n",
    "\n",
    "Extract all the trigrams from the train set (you can use `nltk.trigrams` to do this).\n",
    "For each sentence in the test set, count how many of its trigrams are also found in the train set.\n",
    "Turn this count into a domain similarity measure by dividing it by the number of trigrams in the test sentence.\n",
    "\n",
    "Note: In order for this fraction to be meaningful from the language model's point of view, the edge token must be added to the front of the test sentences and out-of-vocabulary tokens must be replaced with the unknown token.\n",
    "\n",
    "Create a list that maps each sentence's domain similarity to its space saving amount.\n",
    "Plot a scatter plot showing how the domain similarity measure relates to the space saving amount of each test sentence.\n",
    "If there is a correlation between these two measures, then the points in the scatter plot will form approximately into a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5) The scatter plot should not have created a straight line and should show a lot of bias towards very low space saving amounts, regardless of domain similarity.\n",
    "Why is domain similarity not enough for explaining the compressability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Conclusions (10%)\n",
    "\n",
    "Write the following conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1) What is a simple change in the compression algorithm that can be made to increase compression?\n",
    "Do not suggest any fundamental changes; the algorithm must still work by predicting missing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2) Write, in less than 300 words, your interpretation of the results and how you think the model could perform better.\n",
    "You should talk about things like overfitting/underfitting and whether the model is learning anything deep about English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
