{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Other Imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from copy import deepcopy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text compression assignment\n",
    "\n",
    "It is said that you can measure the intelligence of an AI from the amount it can compress a text without information loss.\n",
    "One way to think about this is that, the more a text is predictable, the more words we can leave out of it as we can guess the missing words.\n",
    "On the other hand, the more intelligent an AI is, the more it will find texts to be predictable and so the more words it can leave out and guess.\n",
    "This has led to a competition called the [Hutter Prize](http://prize.hutter1.net/) where the objective is to compress a given text as much as possible.\n",
    "The record for compressing a 1GB text file extracted from a Wikipedia snapshot is about 115MB.\n",
    "The main hurdle here is that the program used to decompress the file must be treated as part of the compressed file, meaning that the program itself must also be small.\n",
    "\n",
    "In this assignment, you're going to be doing something similar using a smaller text file and using neural language models to guess missing words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data processing (10%)\n",
    "\n",
    "You have a train/dev/test split corpus of text from Wikipedia consisting of single sentences.\n",
    "Each sentence is on a separate line and each sentence has been tokenised for you such that tokens are space separated.\n",
    "This means that you only need to split by space to get the tokens.\n",
    "The text has all been lowercased as well.\n",
    "The objective here is to be able to compress the text losslessly, meaning that it can be decompressed back to the original string:\n",
    "\n",
    "$$\\text{decompress}(\\text{compress}(t)) = t$$\n",
    "\n",
    "Do not do any further pre-processing on the text (such as stemming) as it may result in unrecoverable information loss.\n",
    "The test set is what we will be compressing and will not be processed at all as it will be treated as a single big string by the compression/decompression algorithms.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Load the train set and dev set text files into a list of sentences where each sentence is tokenised (by splitting by space).\n",
    "Do not load the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Train Sentences:\n",
      "\n",
      " [['dr.', 'tonie', 'mcdonald', 'is', 'a', 'life', 'long', 'levittown', 'resident', 'who', 'taught', 'and', 'rose', 'through', 'the', 'ranks', 'of', 'the', 'district', 'she', 'now', 'leads', '.'], ['he', 'received', 'his', 'ba', 'in', 'chemistry', ',', 'magna', 'cum', 'laude', ',', 'from', 'amherst', 'college', 'in', '1', '9', '8', '1', '.'], ['the', 'growth', 'of', 'twin', 'cities', 'international', 'airport', 'during', 'the', 'last', 'half', 'of', 'the', 'twentieth', 'century', 'along', 'with', 'additions', 'of', 'land', 'to', 'the', 'fort', 'snelling', 'compound', 'meant', 'further', 'reductions', '.']]\n",
      "\n",
      "Sample Dev Sentences:\n",
      "\n",
      " [['jones', 'viewed', 'the', 'resolution', 'as', 'the', 'framework', ',', 'and', 'not', 'the', 'final', 'solution', ',', 'for', 'enabling', 'librarians', 'to', 'confront', 'issues', 'that', 'hampered', '``', 'human', 'freedom', \"''\", '.'], ['he', 'lives', 'in', 'ottawa', ',', 'where', 'he', 'composes', 'full-time', '.'], ['many', 'natural', 'gas', 'pipelines', 'span', 'the', 'country', \"'s\", 'territory', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Function to load and tokenize sentences from a text file\n",
    "def load_and_tokenize(file_path):\n",
    "    # Open the file with read permissions and UTF-8 encoding\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # Read all lines from the file into a list\n",
    "        sentences = f.readlines()\n",
    "    # Return a list of tokenized sentences\n",
    "    # Each sentence is stripped of leading/trailing whitespaces and then split by spaces\n",
    "    return [sentence.strip().split(' ') for sentence in sentences]\n",
    "\n",
    "# Define the paths to the train and dev text files\n",
    "train_file_path = 'train.txt'\n",
    "dev_file_path = 'dev.txt'\n",
    "\n",
    "# Load and tokenize the sentences from the train set\n",
    "train_sentences = load_and_tokenize(train_file_path)\n",
    "# Load and tokenize the sentences from the dev set\n",
    "dev_sentences = load_and_tokenize(dev_file_path)\n",
    "\n",
    "# Take the first 3 tokenized sentences from the train set as sample data\n",
    "sample_train = train_sentences[:3]\n",
    "# Take the first 3 tokenized sentences from the dev set as sample data\n",
    "sample_dev = dev_sentences[:3]\n",
    "\n",
    "# Print the sample data from the train set to verify the output\n",
    "print(\"Sample Train Sentences:\\n\\n\", sample_train)\n",
    "# Print the sample data from the dev set to verify the output\n",
    "print(\"\\nSample Dev Sentences:\\n\\n\", sample_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Extract a vocabulary consisting of the tokens that occur at least 3 times in the train set and output the size of your vocabulary.\n",
    "Also output the most frequent vocabulary token in the train set, which should be 'the'.\n",
    "Include the edge token, unknown token, and pad token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7874\n",
      "\n",
      "Most Frequent Token: the\n"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each token in the train set\n",
    "token_counter = Counter(token for sentence in train_sentences for token in sentence)\n",
    "\n",
    "# Create a vocabulary by filtering out tokens that appear less than 3 times in the train set\n",
    "vocab = {token for token, count in token_counter.items() if count >= 3}\n",
    "\n",
    "# Define a set of special tokens: Padding, Unknown, and Edge tokens\n",
    "special_tokens = {'<PAD>', '<UNK>', '<EDGE>'}\n",
    "\n",
    "# Add the special tokens to the vocabulary\n",
    "vocab.update(special_tokens)\n",
    "\n",
    "# Convert the vocabulary set to a dictionary with unique integer IDs for each token\n",
    "vocab_dict = {token: index for index, token in enumerate(vocab)}\n",
    "\n",
    "# Optionally, create an index-to-token dictionary for reverse look-up\n",
    "index_to_token = {index: token for token, index in vocab_dict.items()}\n",
    "\n",
    "# Testing: Check if special tokens are in the vocabulary dictionary\n",
    "assert '<PAD>' in vocab_dict, \"'<PAD>' not found in vocab_dict\"\n",
    "assert '<UNK>' in vocab_dict, \"'<UNK>' not found in vocab_dict\"\n",
    "assert '<EDGE>' in vocab_dict, \"'<EDGE>' not found in vocab_dict\"\n",
    "\n",
    "# Calculate the size of the vocabulary by taking the length of the vocab dictionary\n",
    "vocab_size = len(vocab_dict)\n",
    "\n",
    "# Find the most frequent token in the train set\n",
    "most_frequent_token = token_counter.most_common(1)[0][0]\n",
    "\n",
    "# Output the vocabulary size and the most frequent token for verification\n",
    "print(f\"Vocabulary Size: {vocab_size}\\n\")\n",
    "print(f\"Most Frequent Token: {most_frequent_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) Process the loaded token sequences for the train set and dev set using the vocabulary created above in a way that is suitable for a language model, making use of edge tokens, unknown tokens, and pad tokens.\n",
    "Do not do this for the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence 1: ['dr.', 'tonie', 'mcdonald', 'is', 'a', 'life', 'long', 'levittown', 'resident', 'who', 'taught', 'and', 'rose', 'through', 'the', 'ranks', 'of', 'the', 'district', 'she', 'now', 'leads', '.']\n",
      "\n",
      "Token sentence 1 (with special tokens): ['<EDGE>', 'dr.', '<UNK>', 'mcdonald', 'is', 'a', 'life', 'long', '<UNK>', 'resident', 'who', 'taught', 'and', 'rose', 'through', 'the', 'ranks', 'of', 'the', 'district', 'she', 'now', 'leads', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>']\n",
      "\n",
      "Indexed sentence 1: [2479, 6573, 7867, 3319, 5564, 4474, 4067, 723, 7867, 2080, 6472, 4438, 1969, 3812, 1355, 3476, 5849, 1399, 3476, 1534, 4827, 4610, 4118, 5704, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 2479]\n",
      "\n",
      "Original sentence 2: ['he', 'received', 'his', 'ba', 'in', 'chemistry', ',', 'magna', 'cum', 'laude', ',', 'from', 'amherst', 'college', 'in', '1', '9', '8', '1', '.']\n",
      "\n",
      "Token sentence 2 (with special tokens): ['<EDGE>', 'he', 'received', 'his', 'ba', 'in', 'chemistry', ',', '<UNK>', '<UNK>', '<UNK>', ',', 'from', '<UNK>', 'college', 'in', '1', '9', '8', '1', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>']\n",
      "\n",
      "Indexed sentence 2: [2479, 2605, 1446, 2297, 5246, 1632, 6266, 4276, 7867, 7867, 7867, 4276, 3037, 7867, 5083, 1632, 3048, 1527, 2774, 3048, 5704, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 2479]\n",
      "\n",
      "Original sentence 3: ['the', 'growth', 'of', 'twin', 'cities', 'international', 'airport', 'during', 'the', 'last', 'half', 'of', 'the', 'twentieth', 'century', 'along', 'with', 'additions', 'of', 'land', 'to', 'the', 'fort', 'snelling', 'compound', 'meant', 'further', 'reductions', '.']\n",
      "\n",
      "Token sentence 3 (with special tokens): ['<EDGE>', 'the', 'growth', 'of', 'twin', 'cities', 'international', 'airport', 'during', 'the', 'last', 'half', 'of', 'the', 'twentieth', 'century', 'along', 'with', '<UNK>', 'of', 'land', 'to', 'the', 'fort', '<UNK>', 'compound', 'meant', 'further', 'reductions', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EDGE>']\n",
      "\n",
      "Indexed sentence 3: [2479, 3476, 3005, 1399, 7193, 7025, 5761, 6864, 6255, 3476, 2033, 141, 1399, 3476, 5256, 6524, 13, 5240, 7867, 1399, 6312, 1828, 3476, 4662, 7867, 1662, 3885, 2731, 1154, 5704, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 2479]\n",
      "\n",
      "Processed train tensors (first 3 sentences):\n",
      "tensor([[2479, 6573, 7867, 3319, 5564, 4474, 4067,  723, 7867, 2080, 6472, 4438,\n",
      "         1969, 3812, 1355, 3476, 5849, 1399, 3476, 1534, 4827, 4610, 4118, 5704,\n",
      "         7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274,\n",
      "         7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274,\n",
      "         7274, 7274, 7274, 2479],\n",
      "        [2479, 2605, 1446, 2297, 5246, 1632, 6266, 4276, 7867, 7867, 7867, 4276,\n",
      "         3037, 7867, 5083, 1632, 3048, 1527, 2774, 3048, 5704, 7274, 7274, 7274,\n",
      "         7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274,\n",
      "         7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274,\n",
      "         7274, 7274, 7274, 2479],\n",
      "        [2479, 3476, 3005, 1399, 7193, 7025, 5761, 6864, 6255, 3476, 2033,  141,\n",
      "         1399, 3476, 5256, 6524,   13, 5240, 7867, 1399, 6312, 1828, 3476, 4662,\n",
      "         7867, 1662, 3885, 2731, 1154, 5704, 7274, 7274, 7274, 7274, 7274, 7274,\n",
      "         7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274, 7274,\n",
      "         7274, 7274, 7274, 2479]])\n"
     ]
    }
   ],
   "source": [
    "def process_sentences(sentences, vocab_dict, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = max(len(sentence) for sentence in sentences) + 2  # +2 for edge tokens\n",
    "\n",
    "    processed_sentences = []\n",
    "    indexed_sentences = []  # To keep track of indexed sentences\n",
    "    token_sentences = []  # To visualize the tokens including <EDGE>, <UNK>, and <PAD>\n",
    "\n",
    "    # Retrieve the consistent index for the <EDGE> token\n",
    "    edge_index = vocab_dict['<EDGE>']\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Convert tokens to their corresponding indices in the vocabulary\n",
    "        indexed_sentence = [edge_index] + [vocab_dict.get(token, vocab_dict['<UNK>']) for token in sentence]\n",
    "\n",
    "        # Pad the sentence to the max_length with the index for <PAD>\n",
    "        indexed_sentence += [vocab_dict['<PAD>']] * (max_length - len(indexed_sentence) - 1)\n",
    "\n",
    "        # Append the final edge token\n",
    "        indexed_sentence.append(edge_index)\n",
    "\n",
    "        # Convert the indexed sentence to a tensor and add to the list\n",
    "        processed_sentences.append(torch.tensor(indexed_sentence, dtype=torch.long))\n",
    "\n",
    "        # Create the token sentence with special tokens for visualization\n",
    "        token_sentence = ['<EDGE>'] + [token if token in vocab_dict else '<UNK>' for token in sentence]\n",
    "        token_sentence += ['<PAD>'] * (max_length - len(token_sentence) - 1)\n",
    "        token_sentence.append('<EDGE>')\n",
    "        token_sentences.append(token_sentence)\n",
    "\n",
    "        # Save the indexed sentence for inspection\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    # Stack all sentences into a single tensor for batch processing\n",
    "    processed_sentences = torch.stack(processed_sentences)\n",
    "\n",
    "    return processed_sentences, indexed_sentences, token_sentences\n",
    "\n",
    "# Process the sentences and get both processed tensors, indexed sentences, and token sentences\n",
    "processed_train_tensors, indexed_train_sentences, token_train_sentences = process_sentences(train_sentences, vocab_dict)\n",
    "processed_dev_tensors, indexed_dev_sentences, token_dev_sentences = process_sentences(dev_sentences, vocab_dict)\n",
    "\n",
    "# Print out the first few original, indexed, and token sentences for inspection\n",
    "for i in range(3):\n",
    "    print(f\"Original sentence {i+1}: {train_sentences[i]}\\n\")\n",
    "    print(f\"Token sentence {i+1} (with special tokens): {token_train_sentences[i]}\\n\")\n",
    "    print(f\"Indexed sentence {i+1}: {indexed_train_sentences[i]}\\n\")\n",
    "\n",
    "# Additionally, print out the processed tensors for the first few sentences\n",
    "print(\"Processed train tensors (first 3 sentences):\")\n",
    "print(processed_train_tensors[:3])\n",
    "\n",
    "# Create TensorDatasets for the training and development data\n",
    "train_data = TensorDataset(processed_train_tensors[:, :-1], processed_train_tensors[:, 1:])\n",
    "dev_data = TensorDataset(processed_dev_tensors[:, :-1], processed_dev_tensors[:, 1:])\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64  # You can adjust this based on your system's capabilities\n",
    "\n",
    "# Create DataLoaders for the training and development sets\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) Finally, load the test set text file as single string and keep it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length of Test Set: 218470 characters\n",
      "\n",
      "First 100 characters of Test Set:\n",
      "\n",
      " this coincidence enabled freemasons to wear the forget-me-not badge as a secret sign of membership .\n"
     ]
    }
   ],
   "source": [
    "# Function to load the test set into a single string\n",
    "def load_test_set(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        # Remove trailing whitespace, including newlines\n",
    "        return content.rstrip()  \n",
    "\n",
    "# Specify the path to the test text file\n",
    "test_file_path = 'test.txt'\n",
    "\n",
    "# Load the test set using the function\n",
    "test_set_string = load_test_set(test_file_path)\n",
    "\n",
    "# Output the total length of the test set string\n",
    "print(f\"\\nTotal length of Test Set: {len(test_set_string)} characters\\n\")\n",
    "\n",
    "# Output the first 100 characters of the test set for verification\n",
    "print(\"First 100 characters of Test Set:\\n\\n\", test_set_string[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Evaluation tools (10%)\n",
    "\n",
    "We're going to need a function that evaluates our language models as well as a way to test this function before we make the language model.\n",
    "To test the evaluation function, you need to make a mock model which can be used exactly like a language model but that works with some simple rules.\n",
    "This mock model will then be used to check the evaluation, compression, and decompression functions before we've developed the language model.\n",
    "\n",
    "In this assignment, a language model function assumes the following signature:\n",
    "\n",
    "* A parameter `x_indexes` being a tensor that gives the model's input token indexes of a batch of sentences, starting with the edge token.\n",
    "    The tensor is of type `int64` with shape `(batch size, time steps)`.\n",
    "* Returns a tensor of logits predicting which vocabulary token can be the next token after each token in `x_indexes`.\n",
    "    The tensor is of type `float32` with shape `(batch size, time steps, vocab size)`.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) Develop a mock language model.\n",
    "This language model will be a module that predicts the next token after every token using these rules:\n",
    "\n",
    "* If the actual previous token (not the predicted one) was 'the' then predict that the current token is 'dog'.\n",
    "* Otherwise, predict that the current token is 'the'.\n",
    "\n",
    "Remember that it is logits that will be returned by the forward function, not probabilities.\n",
    "**Give the token being predicted a logit of 2 and all other tokens a logit of 0.**\n",
    "The name of this class should be `MockModel`.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Feel free to use `for` loops and `if` statements.\n",
    "* Remember that `x_indexes` is a tensor of previous tokens.\n",
    "    For example, if `x_indexes` is `[[1, 3]]`, this is saying that the first token to predict has a previous token being 1 and the second token to predict has a previous token being 3.\n",
    "\n",
    "Some test code has been provided to check that your mock model is correct.\n",
    "Fix the test code as instructed in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, index_of_edge, index_of_the, index_of_dog):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.index_of_the = index_of_the\n",
    "        self.index_of_dog = index_of_dog\n",
    "\n",
    "    def forward(self, x_indexes):\n",
    "        # Initialize a tensor of zeros for the logits\n",
    "        logits = torch.zeros((x_indexes.size(0), x_indexes.size(1), self.vocab_size), dtype=torch.float32)\n",
    "        \n",
    "        # Go through each token in the batch\n",
    "        for i in range(x_indexes.size(0)):  # Loop over batch size\n",
    "            for j in range(x_indexes.size(1)):  # Loop over time steps\n",
    "                # Rule: If the actual previous token was 'the' then predict 'dog'\n",
    "                if x_indexes[i, j] == self.index_of_the:\n",
    "                    logits[i, j, self.index_of_dog] = 2\n",
    "                else:  # Otherwise, predict 'the'\n",
    "                    logits[i, j, self.index_of_the] = 2\n",
    "                    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "index_of_edge = vocab_dict['<EDGE>']  # Get the index of the edge token\n",
    "index_of_the = vocab_dict['the']  # Get the index of the token 'the'\n",
    "index_of_dog = vocab_dict['dog']  # Get the index of the token 'dog'\n",
    "\n",
    "# Initialize the mock model with correct parameters\n",
    "mock_model = MockModel(vocab_size, index_of_edge, index_of_the, index_of_dog)  \n",
    "\n",
    "# Define the mock x_indexes tensor\n",
    "mock_x_indexes = torch.tensor([\n",
    "    [index_of_edge, index_of_dog, index_of_the, index_of_the],\n",
    "    [index_of_edge, index_of_the, index_of_dog, index_of_dog],\n",
    "], dtype=torch.int64, device=device)\n",
    "\n",
    "# Define the expected logits tensor\n",
    "mock_expected_logits = torch.zeros((2, 4, vocab_size), dtype=torch.float32, device=device)\n",
    "mock_expected_logits[0, 0, index_of_the] = 2\n",
    "mock_expected_logits[0, 1, index_of_the] = 2\n",
    "mock_expected_logits[0, 2, index_of_dog] = 2\n",
    "mock_expected_logits[0, 3, index_of_dog] = 2\n",
    "mock_expected_logits[1, 0, index_of_the] = 2\n",
    "mock_expected_logits[1, 1, index_of_dog] = 2\n",
    "mock_expected_logits[1, 2, index_of_the] = 2\n",
    "mock_expected_logits[1, 3, index_of_the] = 2\n",
    "\n",
    "# Get the logits from the mock model\n",
    "mock_logits = mock_model(mock_x_indexes)\n",
    "# Perform the assertions to ensure the mock model is working correctly\n",
    "assert mock_logits.shape == mock_expected_logits.shape, 'Output shape is invalid.'\n",
    "assert mock_logits.dtype == mock_expected_logits.dtype, 'Output data type is invalid.'\n",
    "assert torch.unique(mock_logits.detach()).tolist() == [0.0, 2.0], 'Output has values other than 0 and 2'\n",
    "assert (mock_logits == mock_expected_logits).all(), 'Output has the wrong logits.'\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) Next, we need a function that measures the perplexity of a language model on the dev set.\n",
    "Your function must take a model and a data set of token indexes and return the perplexity over the entire data set.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Don't forget that the perplexity includes the probability of the edge token at the end of the sentence.\n",
    "* Don't forget to ignore pad tokens.\n",
    "\n",
    "Use this function to find the mock model's perplexity on the dev set, which should be equal to `7062.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on the dev set: 7062.2587890625\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, data_loader, pad_token_id, edge_token_id):\n",
    "    model.eval() \n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            # Ensure inputs are on the correct device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            logits = logits.view(-1, logits.size(-1))  # Reshape for compatibility with targets\n",
    "            \n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            targets = targets.view(-1)  # Flatten targets for gather\n",
    "            target_log_probs = log_probs.gather(dim=1, index=targets.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Exclude padding tokens, but include the edge token at the end of the sentence\n",
    "            mask = (targets != pad_token_id)\n",
    "            for i in range(inputs.size(0)):\n",
    "                mask[i * inputs.size(1) - 1] |= (inputs[i, -1] == edge_token_id)\n",
    "            \n",
    "            target_log_probs = target_log_probs * mask.float()\n",
    "\n",
    "            total_log_prob += target_log_probs.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    average_neg_log_prob = -total_log_prob / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(average_neg_log_prob))\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "# Example usage of the function\n",
    "dev_perplexity = calculate_perplexity(mock_model, dev_loader, vocab_dict['<PAD>'], vocab_dict['<EDGE>'])\n",
    "print(f\"Perplexity on the dev set: {dev_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Compression and decompression (20%)\n",
    "\n",
    "We will now write the code that makes the actual compression and decompression of a text.\n",
    "\n",
    "The compression algorithm will work as follows:\n",
    "\n",
    "* You have a string of text to compress called `text` and a language model called `model`.\n",
    "* Extract a list of tokens from `text` called `tokens` and a list of corresponding token indexes called `indexes`.\n",
    "* Use `model` on `indexes` to produce `predicted`, a list of predicted next tokens for every index in `indexes`.\n",
    "    A predicted next token is just the most probable token according to `model`.\n",
    "* If a token in `predicted` corresponds to a token in `tokens`, then that token can be predicted by the model from its previous tokens.\n",
    "    In this case, we don't need to have the token written down as it can be predicted, so we replace it in `tokens` with the single letter 'X' to say that a token should be predicted here.\n",
    "    If 'X' is shorter than the replaced token, then the text will become shorter.\n",
    "    Since all the text in our data sets is in lowercase, there will never be an 'X' in a sentence, so we can safely use it as a flag.\n",
    "* If the token isn't correctly predicted then we leave the token in the text as-is.\n",
    "* After all predictable tokens in `tokens` have been replaced with an 'X', return `tokens` as a space separated string.\n",
    "\n",
    "The decompression algorithm will work as follows:\n",
    "\n",
    "* You have a string of compressed text called `text` and a language model called `model`.\n",
    "* Extract a list of tokens from `text` called `tokens`.\n",
    "* Go through the tokens in `tokens` from the front and stop at the first 'X'.\n",
    "* Convert all the tokens before the 'X' to token indexes called `indexes`.\n",
    "* Use `model` to predict what the most probable token at the end of `indexes` would be.\n",
    "* Replace the 'X' in `tokens` with this most probable token.\n",
    "* Repeat this for every 'X'.\n",
    "* After all 'X' are replaced in `tokens`, return `tokens` as a space separated string.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) Start with the compression function.\n",
    "The input text will consist of sentences separated by new lines and space separated tokens (just like the raw data sets).\n",
    "The function should return a single string with each line in the input text being compressed.\n",
    "Remember that we want a compressed text to be decompressed back into the exact original text, which means that all out-of-vocabulary tokens must be left as-is (**there must not be any unknown tokens in the output**).\n",
    "\n",
    "Print out the result of compressing this sentence using the mock model:\n",
    "\n",
    "`the dog bit the cat sensually .`\n",
    "\n",
    "which should be compressed into:\n",
    "\n",
    "`X X bit X cat sensually .`\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You don't need to follow the algorithm described above exactly (you can use different variable names and you can use new variables).\n",
    "* Don't forget that out of vocabulary tokens still need to be replaced with the unknown token when creating the token indexes.\n",
    "    What you can't do is return unknown tokens in the compressed output.\n",
    "* The most probable token index for all token positions at once can be found from the logits by using `.argmax(1)`.\n",
    "* Do not compare the token indexes of the uncompressed sentence to the predicted token indexes as otherwise the unknown token can be considered a correct prediction.\n",
    "    Instead, compare the predictions with the string tokens in the uncompressed sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed Sentence: X X bit X cat sensually .\n"
     ]
    }
   ],
   "source": [
    "def compress_text(text, model, vocab_dict, index_to_token):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = text.split('\\n')\n",
    "    tokenized_sentences = [sentence.split(' ') for sentence in sentences]\n",
    "\n",
    "    compressed_sentences = []\n",
    "\n",
    "    for tokens in tokenized_sentences:\n",
    "        # Convert tokens to their corresponding indexes, adding a dummy at the start for the first prediction\n",
    "        indexes = [vocab_dict['<EDGE>']] + [vocab_dict.get(token, vocab_dict['<UNK>']) for token in tokens]\n",
    "\n",
    "        # Convert indexes to a tensor and predict the next tokens using the model\n",
    "        indexes_tensor = torch.tensor([indexes[:-1]], dtype=torch.int64)\n",
    "        logits = model(indexes_tensor)\n",
    "        predicted_indexes = logits.argmax(2).squeeze(0)\n",
    "\n",
    "        # Replace predictable tokens with 'X'\n",
    "        compressed_tokens = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Get the predicted token for the current position\n",
    "            predicted_token = index_to_token[predicted_indexes[i].item()]\n",
    "\n",
    "            if predicted_token == token:\n",
    "                compressed_tokens.append('X')\n",
    "            else:\n",
    "                compressed_tokens.append(token)\n",
    "\n",
    "        # Join the compressed tokens and add to the list of compressed sentences\n",
    "        compressed_sentence = ' '.join(compressed_tokens)\n",
    "        compressed_sentences.append(compressed_sentence)\n",
    "\n",
    "    # Join the compressed sentences into a single string\n",
    "    compressed_text = '\\n'.join(compressed_sentences)\n",
    "    return compressed_text\n",
    "\n",
    "# Compress the example sentence using the mock model\n",
    "example_sentence = \"the dog bit the cat sensually .\"\n",
    "compressed_sentence = compress_text(example_sentence, mock_model, vocab_dict, index_to_token)\n",
    "print(\"Compressed Sentence:\", compressed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) Now write the decompression function.\n",
    "Again, The input text will consist of sentences separated by new lines and space separated tokens, only this time, some of those tokens will be an 'X'.\n",
    "The function should return a single big string where each line in the compressed text is decompressed back into the original input line.\n",
    "\n",
    "Print out the result of decompressing the compressed text:\n",
    "\n",
    "`X X bit X cat sensually .`\n",
    "\n",
    "which should be decompressed into:\n",
    "\n",
    "`the dog bit the cat sensually .`\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You cannot use the language model once to predict all 'X's at once because the sentence prefix leading up to the 'X' must not have another 'X' in it.\n",
    "    So you have to make a separate language model prediction for every 'X' using only the tokens that come before 'X' as input to the language model (plus the edge token at the front).\n",
    "* Don't forget that the input to the language model cannot contain 'X's, so make sure that you're replacing those 'X's with their predicted token when constructing the language model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed Text: the dog bit the cat sensually .\n"
     ]
    }
   ],
   "source": [
    "def decompress_text(text, model, vocab_dict, index_to_token):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = text.split('\\n')\n",
    "    tokenized_sentences = [sentence.split(' ') for sentence in sentences]\n",
    "\n",
    "    decompressed_sentences = []\n",
    "\n",
    "    for tokens in tokenized_sentences:\n",
    "        # Process each token in the sentence\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == 'X':\n",
    "                # Prepare the prefix tokens for prediction (convert to indexes)\n",
    "                prefix_indexes = [vocab_dict['<EDGE>']] + [vocab_dict.get(token, vocab_dict['<UNK>']) for token in tokens[:i]]\n",
    "\n",
    "                # Convert indexes to a tensor and predict the next token using the model\n",
    "                prefix_indexes_tensor = torch.tensor([prefix_indexes], dtype=torch.int64)\n",
    "                logits = model(prefix_indexes_tensor)\n",
    "                predicted_index = logits[0, -1].argmax().item()\n",
    "                predicted_token = index_to_token[predicted_index]\n",
    "\n",
    "                # Replace 'X' with the predicted token\n",
    "                tokens[i] = predicted_token\n",
    "\n",
    "        # Join the decompressed tokens and add to the list of decompressed sentences\n",
    "        decompressed_sentence = ' '.join(tokens)\n",
    "        decompressed_sentences.append(decompressed_sentence)\n",
    "\n",
    "    # Join the decompressed sentences into a single string\n",
    "    decompressed_text = '\\n'.join(decompressed_sentences)\n",
    "    return decompressed_text\n",
    "\n",
    "# Decompress the example compressed text using the mock model\n",
    "compressed_text = \"X X bit X cat sensually .\"\n",
    "decompressed_text = decompress_text(compressed_text, mock_model, vocab_dict, index_to_token)\n",
    "print(\"Decompressed Text:\", decompressed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) Next, calculate and print the space saving amount of the mock model on the test set.\n",
    "The space saving amount is calculated as follows:\n",
    "\n",
    "$$\\text{space\\_saving}(t) = 1 - \\frac{|\\text{compress}(t)|}{|t|}$$\n",
    "\n",
    "where $|t|$ is the number of characters in text $t$.\n",
    "\n",
    "This measure tells you what fraction of the original size has been shaved off after compression (higher is better).\n",
    "The mock model should give 2.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space saving: 2.36%\n"
     ]
    }
   ],
   "source": [
    "def calculate_space_saving(original_text, model, vocab_dict, index_to_token):\n",
    "    # Use the compress_text function to compress the original text\n",
    "    compressed_text = compress_text(original_text, model, vocab_dict, index_to_token)\n",
    "\n",
    "    # Calculate the length of the original and compressed text\n",
    "    original_size = len(original_text)\n",
    "    compressed_size = len(compressed_text)\n",
    "\n",
    "    # Calculate the space saving as a fraction\n",
    "    space_saving_fraction = 1 - (compressed_size / original_size)\n",
    "\n",
    "    return space_saving_fraction\n",
    "\n",
    "# Example usage with the test set string\n",
    "space_saving_fraction = calculate_space_saving(test_set_string, mock_model, vocab_dict, index_to_token)\n",
    "\n",
    "# Print the space saving as a percentage\n",
    "print(f\"Space saving: {space_saving_fraction * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Making and using a language model (50%)\n",
    "\n",
    "Now we finally train a language model and use it to compress the test set.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1) Train a neural language model on the train set.\n",
    "After training, show a graph of how the *dev set perplexity* varies with each epoch (use the perplexity function you wrote above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   5%|‚ñç         | 8/161 [00:16<05:23,  2.11s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15012\\2956521286.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;31m# Backward pass and optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the neural language model architecture\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Slightly adjusted hyperparameters for more complexity\n",
    "embedding_dim = 300  # Increased from 256\n",
    "hidden_dim = 600     # Increased from 512\n",
    "learning_rate = 0.0005  # Decreased for finer updates\n",
    "batch_size = 128  # Increased for potentially faster training\n",
    "num_layers = 3  # One additional layer\n",
    "dropout_prob = 0.6  # Increased dropout for more regularization\n",
    "\n",
    "# Define hyperparameters\n",
    "# embedding_dim = 256  \n",
    "# hidden_dim = 512     \n",
    "# learning_rate = 0.001\n",
    "# batch_size = 64\n",
    "# num_layers = 2\n",
    "# dropout_prob = 0.5 \n",
    "\n",
    "# Instantiate the model with the hyperparameters\n",
    "model = LanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_dict['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "dev_perplexities = []\n",
    "min_delta = 0.01\n",
    "patience = 3\n",
    "wait = 0\n",
    "best_perplexity = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    dev_perplexity = calculate_perplexity(model, dev_loader, vocab_dict['<PAD>'], vocab_dict['<EDGE>'])\n",
    "    dev_perplexities.append(dev_perplexity)\n",
    "    print(f\"Epoch {epoch + 1}: Dev Perplexity: {dev_perplexity}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if dev_perplexity < best_perplexity - min_delta:\n",
    "        best_perplexity = dev_perplexity\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "# Load the best model if early stopping was triggered\n",
    "if best_model is not None:\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "# Plotting the perplexity for the development set only\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(dev_perplexities) + 1), dev_perplexities, label='Dev Perplexity', color='blue', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Development Set Perplexity Across Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, len(dev_perplexities) + 1))  # Ensure x-axis ticks match the number of epochs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2) Now measure the space saving amount of the trained model on the test text.\n",
    "Also check that when you decompress the compressed test text, you get exactly the same string as the test text.\n",
    "\n",
    "Note: You may need to strip off the new line character from the end of the test text when comparing it to the decompressed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the test text\n",
    "compressed_test_text = compress_text(test_set_string, model, vocab_dict, index_to_token)\n",
    "\n",
    "# Calculate the space saving\n",
    "space_saving = calculate_space_saving(test_set_string, model, vocab_dict, index_to_token)\n",
    "print(f\"Space saving: {space_saving * 100:.2f}%\")\n",
    "\n",
    "# Decompress the compressed text\n",
    "decompressed_test_text = decompress_text(compressed_test_text, model, vocab_dict, index_to_token)\n",
    "\n",
    "# Compare the original and decompressed texts\n",
    "if test_set_string.strip() == decompressed_test_text.strip():\n",
    "    print(\"Decompression successful: Original and decompressed texts are the same.\")\n",
    "else:\n",
    "    print(\"Decompression mismatch: There is a difference between the original and decompressed texts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample sentence to compress and decompress\n",
    "sample_sentence = \"6 % of all households were made up of individuals , and 1 5 .\"\n",
    "\n",
    "# Compress the sample sentence using the trained model\n",
    "compressed_sample = compress_text(sample_sentence, model, vocab_dict, index_to_token)\n",
    "print(f\"Compressed Sample Sentence: {compressed_sample}\")\n",
    "\n",
    "# Decompress the compressed sample sentence\n",
    "decompressed_sample = decompress_text(compressed_sample, model, vocab_dict, index_to_token)\n",
    "print(f\"Decompressed Sample Sentence: {decompressed_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3) Now you need to analyse the model's output.\n",
    "Split the test text into sentences and compress each individual sentence.\n",
    "Print out the top 5 most compressed sentences and the top 5 least compressed sentences according to the space saving metric together with the compressed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test text into sentences\n",
    "sentences = sent_tokenize(test_set_string)\n",
    "\n",
    "# Compress each sentence and calculate space saving\n",
    "sentence_compressions = []\n",
    "for sentence in sentences:\n",
    "    compressed_sentence = compress_text(sentence, model, vocab_dict, index_to_token)\n",
    "    space_saving = calculate_space_saving(sentence, model, vocab_dict, index_to_token)  # Updated function call\n",
    "    sentence_compressions.append((sentence, compressed_sentence, space_saving))\n",
    "\n",
    "# Sort the sentences by space saving\n",
    "sentence_compressions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Extract top 5 most and least compressed sentences\n",
    "top_5_most_compressed = sentence_compressions[:5]\n",
    "top_5_least_compressed = sentence_compressions[-5:]\n",
    "\n",
    "# Print out the top 5 most compressed sentences\n",
    "print(\"Top 5 Most Compressed Sentences:\\n\")\n",
    "for original, compressed, saving in top_5_most_compressed:\n",
    "    print(f\"Original: {original}\\nCompressed: {compressed}\\nSpace Saving: {saving * 100:.2f}%\\n\")\n",
    "\n",
    "print(\"-\"*125)\n",
    "\n",
    "# Print out the top 5 least compressed sentences\n",
    "print(\"Top 5 Least Compressed Sentences:\\n\")\n",
    "for original, compressed, saving in top_5_least_compressed:\n",
    "    print(f\"Original: {original}\\nCompressed: {compressed}\\nSpace Saving: {saving * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4) Is the reason for whether a sentence is compressible or not due to its similarity to the train set (a sentence that is similar to one in the train set would be easier to predict and thus more tokens will be compressed)?\n",
    "Find out the answer to this by doing the following:\n",
    "\n",
    "Extract all the trigrams from the train set (you can use `nltk.trigrams` to do this).\n",
    "For each sentence in the test set, count how many of its trigrams are also found in the train set.\n",
    "Turn this count into a domain similarity measure by dividing it by the number of trigrams in the test sentence.\n",
    "\n",
    "Note: In order for this fraction to be meaningful from the language model's point of view, the edge token must be added to the front of the test sentences and out-of-vocabulary tokens must be replaced with the unknown token.\n",
    "\n",
    "Create a list that maps each sentence's domain similarity to its space saving amount.\n",
    "Plot a scatter plot showing how the domain similarity measure relates to the space saving amount of each test sentence.\n",
    "If there is a correlation between these two measures, then the points in the scatter plot will form approximately into a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5) The scatter plot should not have created a straight line and should show a lot of bias towards very low space saving amounts, regardless of domain similarity.\n",
    "Why is domain similarity not enough for explaining the compressability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Conclusions (10%)\n",
    "\n",
    "Write the following conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1) What is a simple change in the compression algorithm that can be made to increase compression?\n",
    "Do not suggest any fundamental changes; the algorithm must still work by predicting missing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2) Write, in less than 300 words, your interpretation of the results and how you think the model could perform better.\n",
    "You should talk about things like overfitting/underfitting and whether the model is learning anything deep about English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
